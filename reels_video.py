"""ë¦´ìŠ¤ ì˜ìƒ í•©ì„± ëª¨ë“ˆ â€” 1ë¶„ê±´ê°•í†¡.

ì°¸ê³  ì˜ìƒ ìŠ¤íƒ€ì¼: ìƒë‹¨ 55% GIF/ì´ë¯¸ì§€ + í•˜ë‹¨ 45% ë¸Œëœë“œ ë¸”ë£¨ í…ìŠ¤íŠ¸.
GIFëŠ” ê½‰ ì±„ìš°ì§€ ì•Šê³  ìƒë‹¨ ì˜ì—­ì— ë°°ì¹˜. ì˜¤í”„ë‹ ì—†ì´ ë³¸ë¡ ë¶€í„° ì‹œì‘.
ë‚˜ë ˆì´ì…˜(edge-tts) ê¸°ë°˜ ë™ì  ì”¬ êµ¬ì„± â†’ BUMPER.mov ì—°ê²°.
"""
from __future__ import annotations

import asyncio
import io
import logging
import os
import tempfile
from pathlib import Path

import edge_tts
import numpy as np
from moviepy import (
    AudioFileClip,
    CompositeVideoClip,
    ImageClip,
    VideoClip,
    VideoFileClip,
    concatenate_videoclips,
    CompositeAudioClip,
)
from PIL import Image

logger = logging.getLogger(__name__)

# â”€â”€ ê²½ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_ASSETS_DIR = os.path.join(os.path.dirname(__file__), "assets", "1min_health")
INTRO_PATH = os.path.join(_ASSETS_DIR, "INTRO.mp4")
BUMPER_PATH = os.path.join(_ASSETS_DIR, "BUMPER.mov")

# â”€â”€ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
W, H = 1080, 1920  # 9:16
FPS = 30
TRANSITION_DUR = 0.4
SLIDE_PADDING = 0.5
MEDIA_RATIO = 0.55  # GIF/ì´ë¯¸ì§€ê°€ ì°¨ì§€í•˜ëŠ” ìƒë‹¨ ë¹„ìœ¨ (ì°¸ê³  ì˜ìƒ ê¸°ì¤€)
MEDIA_H = int(H * MEDIA_RATIO)  # ~1056px
BRAND_BLUE = (43, 91, 224)

# â”€â”€ temp íŒŒì¼ ì¶”ì  (MoviePy ë Œë”ë§ ì™„ë£Œ í›„ ì •ë¦¬) â”€â”€â”€â”€â”€â”€â”€â”€
_temp_files: list[str] = []


def _track_temp(path: str) -> str:
    """temp íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ì  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€."""
    _temp_files.append(path)
    return path


def _cleanup_temp_files():
    """ì¶”ì ëœ temp íŒŒì¼ ì¼ê´„ ì •ë¦¬."""
    for path in _temp_files:
        try:
            os.unlink(path)
        except Exception:
            pass
    _temp_files.clear()

# â”€â”€ ìŒì„± í”„ë¦¬ì…‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VOICES = {
    "ì—¬ì„± (ì„ íˆ)": "ko-KR-SunHiNeural",
    "ë‚¨ì„± (í˜„ìˆ˜)": "ko-KR-HyunsuMultilingualNeural",
    "ë‚¨ì„± (ì¸ì¤€)": "ko-KR-InJoonNeural",
}
DEFAULT_VOICE = "ko-KR-HyunsuMultilingualNeural"

# ìŒì„±ë³„ ìµœì  rate/pitch ì„¤ì • (ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬)
_VOICE_PRESETS = {
    "ko-KR-SunHiNeural": {"rate": "-8%", "pitch": "+5Hz"},
    "ko-KR-HyunsuMultilingualNeural": {"rate": "-5%", "pitch": "+0Hz"},
    "ko-KR-InJoonNeural": {"rate": "-5%", "pitch": "+0Hz"},
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë‚˜ë ˆì´ì…˜ ìƒì„± (edge-tts)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _preprocess_narration(text: str) -> str:
    """ë‚˜ë ˆì´ì…˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ â€” ìì—°ìŠ¤ëŸ¬ìš´ TTSë¥¼ ìœ„í•œ ë³´ì •.

    - ì´ëª¨ì§€ ì œê±° (TTSê°€ ì½ìœ¼ë©´ ì–´ìƒ‰)
    - ã…‹ã…‹ ë“± ì›ƒìŒ í‘œí˜„ ì œê±°
    - ë§ˆì¹¨í‘œ ë’¤ ì‰¼í‘œ ì¶”ê°€ (ìì—°ìŠ¤ëŸ¬ìš´ í˜¸í¡)
    """
    import re
    # ì´ëª¨ì§€ ì œê±°
    text = re.sub(r'[\U0001F600-\U0001F9FF\U00002702-\U000027B0'
                  r'\U0001F1E0-\U0001F1FF\U00002600-\U000026FF'
                  r'\U0000FE00-\U0000FE0F\U0001FA00-\U0001FAFF]+', '', text)
    # ã…‹ã…‹, ã„·ã„· ë“± ì œê±°
    text = re.sub(r'[ã…‹ã…ã„·ã… ã…œ]{2,}', '', text)
    # ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    return text


async def _generate_narration_async(text: str, output_path: str,
                                     voice: str = DEFAULT_VOICE) -> str:
    text = _preprocess_narration(text)
    preset = _VOICE_PRESETS.get(voice, {})
    communicate = edge_tts.Communicate(
        text, voice,
        rate=preset.get("rate", "-5%"),
        pitch=preset.get("pitch", "+0Hz"),
    )
    await communicate.save(output_path)
    return output_path


def generate_narration(text: str, output_path: str,
                       voice: str = DEFAULT_VOICE) -> str:
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            import nest_asyncio
            nest_asyncio.apply()
            loop.run_until_complete(
                _generate_narration_async(text, output_path, voice))
        else:
            loop.run_until_complete(
                _generate_narration_async(text, output_path, voice))
    except RuntimeError:
        asyncio.run(_generate_narration_async(text, output_path, voice))
    return output_path


def generate_narrations(slides: list[dict], tmp_dir: str,
                        voice: str = DEFAULT_VOICE) -> list[str]:
    paths = []
    for i, slide in enumerate(slides):
        narration = slide.get("narration", "")
        if not narration.strip():
            paths.append("")
            continue
        out = os.path.join(tmp_dir, f"narration_{i}.mp3")
        try:
            generate_narration(narration, out, voice)
            paths.append(out)
            logger.info(f"ë‚˜ë ˆì´ì…˜ ìƒì„± ì™„ë£Œ: slide_{i} â†’ {out}")
        except Exception as e:
            logger.error(f"ë‚˜ë ˆì´ì…˜ ìƒì„± ì‹¤íŒ¨ (slide_{i}): {e}")
            paths.append("")
    return paths


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì˜¤ë””ì˜¤ ìœ í‹¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_audio_duration(audio_path: str) -> float:
    if not audio_path or not os.path.exists(audio_path):
        return 3.0
    try:
        clip = AudioFileClip(audio_path)
        dur = clip.duration
        clip.close()
        return dur
    except Exception:
        return 3.0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í´ë¦½ ìœ í‹¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fit_clip_to_reel(clip):
    """ì˜ìƒ/ì´ë¯¸ì§€ í´ë¦½ì„ 1080Ã—1920ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ+í¬ë¡­."""
    cw, ch = clip.size
    target_ratio = W / H

    if cw / ch > target_ratio:
        new_h = H
        new_w = int(cw * (H / ch))
    else:
        new_w = W
        new_h = int(ch * (W / cw))

    resized = clip.resized((new_w, new_h))
    x_center = new_w / 2
    y_center = new_h / 2
    cropped = resized.cropped(
        x1=x_center - W / 2, y1=y_center - H / 2,
        x2=x_center + W / 2, y2=y_center + H / 2,
    )
    return cropped


def _letterbox_landscape(clip, bg_color=(43, 91, 224)):
    """ê°€ë¡œ ì˜ìƒì„ ì„¸ë¡œ í”„ë ˆì„ ì•ˆì— ë ˆí„°ë°•ìŠ¤ë¡œ ë°°ì¹˜ (ì›ë³¸ ë¹„ìœ¨ ìœ ì§€).

    ê°€ë¡œ 1920Ã—1080 â†’ ì„¸ë¡œ 1080Ã—1920 ì•ˆì—ì„œ:
      - ì˜ìƒì„ ê°€ë¡œí­ 1080ì— ë§ê²Œ ì¶•ì†Œ (1080Ã—607)
      - ìƒí•˜ ë¸Œëœë“œ ë¸”ë£¨ ë°°ê²½ìœ¼ë¡œ íŒ¨ë”©
    """
    cw, ch = clip.size

    # ê°€ë¡œí­ = Wì— ë§ì¶”ê³ , ì„¸ë¡œëŠ” ë¹„ìœ¨ ìœ ì§€
    scale = W / cw
    new_w = W
    new_h = int(ch * scale)
    resized = clip.resized((new_w, new_h))

    # ë¸Œëœë“œ ë¸”ë£¨ ë°°ê²½
    bg_arr = np.full((H, W, 3), bg_color, dtype=np.uint8)
    bg_clip = ImageClip(bg_arr).with_duration(clip.duration)

    # ì„¸ë¡œ ì¤‘ì•™ ë°°ì¹˜
    y_offset = (H - new_h) // 2
    final = CompositeVideoClip(
        [bg_clip, resized.with_position(("center", y_offset))],
        size=(W, H),
    ).with_duration(clip.duration)

    # ì›ë³¸ ì˜¤ë””ì˜¤ ìœ ì§€
    if clip.audio is not None:
        final = final.with_audio(clip.audio)

    return final


def _image_bytes_to_clip(img_bytes: bytes, duration: float) -> ImageClip:
    """PNG bytes â†’ MoviePy ImageClip (RGB)."""
    img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
    arr = np.array(img)
    return ImageClip(arr).with_duration(duration)


def _overlay_png_to_clip(png_bytes: bytes, duration: float) -> ImageClip:
    """íˆ¬ëª… PNG bytes â†’ MoviePy ImageClip (RGBA ë§ˆìŠ¤í¬ ì§€ì›)."""
    img = Image.open(io.BytesIO(png_bytes)).convert("RGBA")
    arr = np.array(img)
    # RGBAì—ì„œ RGB + mask ë¶„ë¦¬
    rgb = arr[:, :, :3]
    alpha = arr[:, :, 3] / 255.0  # 0~1 ë²”ìœ„
    clip = ImageClip(rgb).with_duration(duration)
    mask = ImageClip(alpha, is_mask=True).with_duration(duration)
    clip = clip.with_mask(mask)
    return clip


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GIF/ì˜ìƒ â†’ ìƒë‹¨ 55% ì”¬ í´ë¦½ (ì°¸ê³  ì˜ìƒ ìŠ¤íƒ€ì¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fit_to_area(clip, target_w: int, target_h: int):
    """í´ë¦½ì„ target_w Ã— target_h ì˜ì—­ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ+í¬ë¡­."""
    cw, ch = clip.size
    target_ratio = target_w / target_h

    if cw / ch > target_ratio:
        new_h = target_h
        new_w = int(cw * (target_h / ch))
    else:
        new_w = target_w
        new_h = int(ch * (target_w / cw))

    resized = clip.resized((new_w, new_h))
    x_center = new_w / 2
    y_center = new_h / 2
    return resized.cropped(
        x1=x_center - target_w / 2, y1=y_center - target_h / 2,
        x2=x_center + target_w / 2, y2=y_center + target_h / 2,
    )


def _load_video_clip(media_bytes: bytes, media_info: dict, duration: float):
    """ë¯¸ë””ì–´ bytes â†’ ë£¨í•‘ VideoFileClip (temp íŒŒì¼ ì¶”ì ).

    ì£¼ì˜: temp íŒŒì¼ì€ MoviePy ë Œë”ë§ì´ ëë‚  ë•Œê¹Œì§€ ìœ ì§€!
    """
    is_mp4 = media_info.get("mp4_url", "").endswith(".mp4") or b"ftyp" in media_bytes[:20]
    suffix = ".mp4" if is_mp4 else ".gif"

    tmp = tempfile.NamedTemporaryFile(suffix=suffix, delete=False)
    tmp.write(media_bytes)
    tmp.close()
    _track_temp(tmp.name)

    clip = VideoFileClip(tmp.name)
    # ë£¨í•‘ (ìµœëŒ€ 3íšŒ â€” ë©”ëª¨ë¦¬ ì ˆì•½)
    if clip.duration < duration:
        n_loops = min(int(duration / clip.duration) + 1, 3)
        clip = concatenate_videoclips([clip] * n_loops).subclipped(0, duration)
    else:
        clip = clip.subclipped(0, min(clip.duration, duration))
    return clip


def _media_to_scene_clip(media_bytes: bytes | None, media_info: dict | None,
                          overlay_png: bytes | None, duration: float):
    """ë¯¸ë””ì–´ + ì˜¤ë²„ë ˆì´ â†’ ì”¬ í´ë¦½ (PIL ê¸°ë°˜ í•©ì„± â€” ì‹ ë¢°ì„± ë³´ì¥).

    MoviePyì˜ CompositeVideoClip + ë§ˆìŠ¤í¬ ì•ŒíŒŒ í•©ì„± ëŒ€ì‹ ,
    PIL.Image.alpha_composite()ë¡œ ë§¤ í”„ë ˆì„ì„ ì§ì ‘ í•©ì„±.
    GIF/ì˜ìƒì´ í™•ì‹¤íˆ ë³´ì´ë„ë¡ ë³´ì¥.

    ë ˆì´ì•„ì›ƒ:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ [ğŸ”´í†¡]   n/N    â”‚  â† ì˜¤ë²„ë ˆì´ (íˆ¬ëª… ë°°ê²½, ë¡œê³ Â·ë²ˆí˜¸)
    â”‚                 â”‚
    â”‚  GIF/ì´ë¯¸ì§€     â”‚  â† ìƒë‹¨ 55% (MEDIA_H px)
    â”‚  (ì›ë³¸ë¹„ìœ¨í¬ë¡­)  â”‚
    â”‚                 â”‚
    â”œâ”€ ê·¸ë¼ë°ì´ì…˜ â”€â”€â”€â”€â”¤
    â”‚  â–  ë¸”ë£¨ â– â– â– â– â–   â”‚  â† í•˜ë‹¨ 45% (ë¶ˆíˆ¬ëª… ë¸”ë£¨)
    â”‚  display_text   â”‚  â† ì˜¤ë²„ë ˆì´ í…ìŠ¤íŠ¸
    â”‚  @1ë¶„ê±´ê°•í†¡     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    media_type = media_info.get("type", "image") if media_info else "none"

    # â”€â”€ ì˜¤ë²„ë ˆì´ RGBA ì´ë¯¸ì§€ ì¤€ë¹„ (1íšŒ) â”€â”€
    overlay_pil = None
    if overlay_png:
        try:
            overlay_pil = Image.open(io.BytesIO(overlay_png)).convert("RGBA")
        except Exception as e:
            logger.warning(f"ì˜¤ë²„ë ˆì´ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _composite_frame(base_rgb: Image.Image) -> np.ndarray:
        """RGB ë°°ê²½ ìœ„ì— RGBA ì˜¤ë²„ë ˆì´ë¥¼ ì•ŒíŒŒ í•©ì„± â†’ RGB numpy."""
        if overlay_pil:
            base_rgba = base_rgb.convert("RGBA")
            composited = Image.alpha_composite(base_rgba, overlay_pil)
            return np.array(composited.convert("RGB"))
        return np.array(base_rgb)

    # â”€â”€ Case 1: GIF/Video â†’ per-frame PIL í•©ì„± â”€â”€
    if media_type in ("gif", "video") and media_bytes:
        try:
            raw_clip = _load_video_clip(media_bytes, media_info, duration)
            fitted_clip = _fit_to_area(raw_clip, W, MEDIA_H)
            logger.info(f"ë¯¸ë””ì–´ í´ë¦½ ë¡œë“œ ì„±ê³µ: {media_type}/{media_info.get('source', '?')} "
                        f"({fitted_clip.size[0]}x{fitted_clip.size[1]}, {fitted_clip.duration:.1f}s)")

            def make_frame(t):
                canvas = Image.new("RGB", (W, H), BRAND_BLUE)
                try:
                    frame_arr = fitted_clip.get_frame(t)
                    frame_img = Image.fromarray(frame_arr)
                    canvas.paste(frame_img, (0, 0))
                except Exception as e:
                    logger.debug(f"í”„ë ˆì„ {t:.2f}s ë¡œë“œ ì‹¤íŒ¨: {e}")
                return _composite_frame(canvas)

            scene = VideoClip(make_frame, duration=duration)
            scene = scene.with_fps(FPS)
            return scene
        except Exception as e:
            logger.warning(f"GIF/Video ì”¬ ìƒì„± ì‹¤íŒ¨ ({media_type}): {e}")

    # â”€â”€ Case 2: Image â†’ PIL í•©ì„± í›„ ImageClip â”€â”€
    if media_type == "image" and media_bytes:
        try:
            canvas = Image.new("RGB", (W, H), BRAND_BLUE)
            img = Image.open(io.BytesIO(media_bytes)).convert("RGB")
            fitted = _fit_cover_pil(img, W, MEDIA_H)
            canvas.paste(fitted, (0, 0))
            logger.info(f"ì´ë¯¸ì§€ í´ë¦½ ìƒì„±: {media_info.get('source', '?')}")
            return ImageClip(_composite_frame(canvas)).with_duration(duration)
        except Exception as e:
            logger.warning(f"ì´ë¯¸ì§€ ì”¬ ìƒì„± ì‹¤íŒ¨: {e}")

    # â”€â”€ Case 3: ë¯¸ë””ì–´ ì—†ìŒ â†’ ë¸”ë£¨ ë°°ê²½ + ì˜¤ë²„ë ˆì´ â”€â”€
    logger.info("ë¯¸ë””ì–´ ì—†ìŒ â†’ ë¸Œëœë“œ ë¸”ë£¨ ë°°ê²½")
    canvas = Image.new("RGB", (W, H), BRAND_BLUE)
    return ImageClip(_composite_frame(canvas)).with_duration(duration)


def _fit_cover_pil(img: Image.Image, w: int, h: int) -> Image.Image:
    """PIL ì´ë¯¸ì§€ë¥¼ wÃ—hì— ë§ê²Œ ì»¤ë²„ í¬ë¡­."""
    pw, ph = img.size
    target_ratio = w / h
    if pw / ph > target_ratio:
        new_w = int(ph * target_ratio)
        left = (pw - new_w) // 2
        img = img.crop((left, 0, left + new_w, ph))
    else:
        new_h = int(pw / target_ratio)
        top = (ph - new_h) // 2
        img = img.crop((0, top, pw, top + new_h))
    return img.resize((w, h), Image.LANCZOS)


def _solid_color_clip(duration: float, color=None):
    """ë‹¨ìƒ‰ ë°°ê²½ í´ë¦½ (í´ë°±ìš©)."""
    c = color or BRAND_BLUE
    arr = np.full((H, W, 3), c, dtype=np.uint8)
    return ImageClip(arr).with_duration(duration)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìŠ¤ì™€ì´í”„ ì „í™˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _swipe_transition(clip1, clip2, trans_dur: float = TRANSITION_DUR):
    w = W

    def pos1(t):
        progress = t / trans_dur
        return (-w * progress, 0)

    def pos2(t):
        progress = t / trans_dur
        return (w - w * progress, 0)

    c1 = clip1.with_duration(trans_dur).with_position(pos1)
    c2 = clip2.with_duration(trans_dur).with_position(pos2)
    return CompositeVideoClip([c1, c2], size=(W, H)).with_duration(trans_dur)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ í•©ì„± (GIF/ì˜ìƒ ë°°ê²½ + í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _render_one_scene(scene_clip, narr_path: str, output_file: str,
                       fade_dur: float = 0.3):
    """ì”¬ í•˜ë‚˜ë¥¼ ê°œë³„ MP4ë¡œ ë Œë”ë§ (ë©”ëª¨ë¦¬ ìµœì†Œí™”).

    í•œ ë²ˆì— í•œ ì”¬ë§Œ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ê³  ë°”ë¡œ ë””ìŠ¤í¬ì— ê¸°ë¡.
    """
    import gc
    narr_dur = get_audio_duration(narr_path)
    slide_dur = narr_dur + SLIDE_PADDING

    # duration ì¡°ì • â€” VideoClip(make_frame)ì€ subclipped ëŒ€ì‹  with_duration ì‚¬ìš©
    clip = scene_clip.with_duration(slide_dur)

    # fps ë³´ì¥ (VideoClipì—ì„œ fpsê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
    if not hasattr(clip, 'fps') or clip.fps is None:
        clip = clip.with_fps(FPS)

    # í˜ì´ë“œ ì¸/ì•„ì›ƒ (ë¶€ë“œëŸ¬ìš´ ì „í™˜ íš¨ê³¼)
    if slide_dur > fade_dur * 2:
        try:
            from moviepy.video.fx import FadeIn, FadeOut
            clip = clip.with_effects([FadeIn(fade_dur), FadeOut(fade_dur)])
        except Exception as e:
            logger.debug(f"í˜ì´ë“œ íš¨ê³¼ ì‹¤íŒ¨: {e}")

    # ë‚˜ë ˆì´ì…˜ ì˜¤ë””ì˜¤ í•©ì„±
    if narr_path and os.path.exists(narr_path):
        audio = AudioFileClip(narr_path)
        clip = clip.with_audio(audio)

    # SFX: ì”¬ ì‹œì‘ì— whoosh
    try:
        from sfx import generate_sfx
        sfx_path = generate_sfx("whoosh")
        if sfx_path and os.path.exists(sfx_path):
            sfx_clip = AudioFileClip(sfx_path)
            if clip.audio is not None:
                clip = clip.with_audio(
                    CompositeAudioClip([clip.audio, sfx_clip.with_start(0.0)])
                )
            _track_temp(sfx_path)
    except Exception:
        pass

    # ê°œë³„ MP4ë¡œ ì €ì¥
    logger.info(f"ì”¬ ë Œë”ë§: {output_file} (dur={slide_dur:.1f}s, fps={clip.fps})")
    clip.write_videofile(
        output_file, fps=FPS, codec="libx264", audio_codec="aac",
        threads=2, preset="fast", logger=None,
    )
    clip.close()
    gc.collect()


def compose_reel(
    scene_clips: list,
    narration_paths: list[str],
    output_path: str,
    include_intro: bool = False,
    include_bumper: bool = True,
    progress_callback=None,
) -> str:
    """ë¦´ìŠ¤ ì˜ìƒ í•©ì„± â€” ì”¬ë³„ ê°œë³„ ë Œë”ë§ + ffmpeg ì—°ê²°.

    ë©”ëª¨ë¦¬ ìµœì í™”: í•œ ë²ˆì— í•œ ì”¬ë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œ.
    í’ˆì§ˆ: preset=fast, í˜ì´ë“œ ì „í™˜, SFX í¬í•¨.
    """
    import gc
    import subprocess

    def _progress(step, total, msg):
        if progress_callback:
            progress_callback(step, total, msg)
        logger.info(f"[{step}/{total}] {msg}")

    output_dir = os.path.dirname(output_path)
    segment_files = []
    total_steps = len(scene_clips) + 3
    current_step = 0

    # INTRO
    if include_intro and os.path.exists(INTRO_PATH):
        current_step += 1
        _progress(current_step, total_steps, "ì¸íŠ¸ë¡œ ì˜ìƒ ì¤€ë¹„ ì¤‘...")
        intro_out = os.path.join(output_dir, "seg_intro.mp4")
        intro_clip = VideoFileClip(INTRO_PATH)
        iw, ih = intro_clip.size
        if iw > ih:
            intro_clip = _letterbox_landscape(intro_clip)
        elif (iw, ih) != (W, H):
            intro_clip = _fit_clip_to_reel(intro_clip)
        intro_clip.write_videofile(
            intro_out, fps=FPS, codec="libx264", audio_codec="aac",
            threads=2, preset="fast", logger=None,
        )
        intro_clip.close()
        gc.collect()
        segment_files.append(intro_out)
    else:
        current_step += 1

    # ê° ì”¬ì„ ê°œë³„ MP4ë¡œ ë Œë”ë§ (í•µì‹¬: í•œ ë²ˆì— í•˜ë‚˜ë§Œ!)
    for i, scene_clip in enumerate(scene_clips):
        current_step += 1
        _progress(current_step, total_steps, f"ì”¬ {i + 1}/{len(scene_clips)} ë Œë”ë§ ì¤‘...")

        narr_path = narration_paths[i] if i < len(narration_paths) else ""
        seg_file = os.path.join(output_dir, f"seg_{i:02d}.mp4")

        try:
            _render_one_scene(scene_clip, narr_path, seg_file)
            segment_files.append(seg_file)
        except Exception as e:
            logger.warning(f"ì”¬ {i} ë Œë”ë§ ì‹¤íŒ¨: {e}")

    # BUMPER
    if include_bumper and os.path.exists(BUMPER_PATH):
        current_step += 1
        _progress(current_step, total_steps, "ë²”í¼ ì˜ìƒ ì¤€ë¹„ ì¤‘...")
        bumper_out = os.path.join(output_dir, "seg_bumper.mp4")
        bumper_clip = VideoFileClip(BUMPER_PATH)
        bw, bh = bumper_clip.size
        if (bw, bh) != (W, H):
            if bw > bh:
                bumper_clip = _letterbox_landscape(bumper_clip)
            else:
                bumper_clip = _fit_clip_to_reel(bumper_clip)
        bumper_clip.write_videofile(
            bumper_out, fps=FPS, codec="libx264", audio_codec="aac",
            threads=2, preset="fast", logger=None,
        )
        bumper_clip.close()
        gc.collect()
        segment_files.append(bumper_out)
    else:
        current_step += 1

    if not segment_files:
        raise ValueError("í•©ì„±í•  ìŠ¬ë¼ì´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ffmpeg concatìœ¼ë¡œ ì—°ê²° (ë©”ëª¨ë¦¬ ê±°ì˜ ì•ˆ ì”€!)
    _progress(total_steps, total_steps, "MP4 ìµœì¢… ì—°ê²° ì¤‘...")
    concat_list = os.path.join(output_dir, "concat.txt")
    with open(concat_list, "w") as f:
        for sf in segment_files:
            f.write(f"file '{sf}'\n")

    cmd = [
        "ffmpeg", "-y", "-f", "concat", "-safe", "0",
        "-i", concat_list, "-c", "copy", output_path,
    ]
    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        logger.error(f"ffmpeg concat ì‹¤íŒ¨: {result.stderr[:500]}")
        # í´ë°±: ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ë¼ë„ ë°˜í™˜
        if segment_files:
            import shutil
            shutil.copy2(segment_files[0], output_path)

    # ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ ì •ë¦¬
    for sf in segment_files:
        try:
            os.unlink(sf)
        except Exception:
            pass
    try:
        os.unlink(concat_list)
    except Exception:
        pass

    # GIF/ì˜ìƒ temp íŒŒì¼ ì •ë¦¬
    _cleanup_temp_files()

    logger.info(f"ë¦´ìŠ¤ ì˜ìƒ ìƒì„± ì™„ë£Œ: {output_path}")
    return output_path


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í†µí•© íŒŒì´í”„ë¼ì¸ (GIF/ì˜ìƒ ë°°ê²½)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_reel(
    slides: list[dict],
    media_data: list[tuple[bytes | None, dict | None]],
    overlay_images: list[bytes],
    output_dir: str | None = None,
    voice: str = DEFAULT_VOICE,
    include_intro: bool = False,
    include_bumper: bool = True,
    progress_callback=None,
) -> dict:
    """ë¦´ìŠ¤ ìƒì„± í†µí•© íŒŒì´í”„ë¼ì¸ (GIF ìƒë‹¨ 55% + ë¸”ë£¨ë°” í•˜ë‹¨ 45%).

    Args:
        slides: ìŠ¤í¬ë¦½íŠ¸ ìŠ¬ë¼ì´ë“œ ë¦¬ìŠ¤íŠ¸
        media_data: [(bytes, metadata), ...] ìŠ¬ë¼ì´ë“œë³„ ë¯¸ë””ì–´ ë°ì´í„°
        overlay_images: í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ PNG bytes
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        voice: TTS ìŒì„± ID
        include_intro: ì¸íŠ¸ë¡œ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ False â€” ë³¸ë¡ ë¶€í„° ì‹œì‘)
    """
    if output_dir is None:
        output_dir = tempfile.mkdtemp(prefix="reel_")
    os.makedirs(output_dir, exist_ok=True)

    # Phase 1: ë‚˜ë ˆì´ì…˜
    if progress_callback:
        progress_callback(0.0, "ë‚˜ë ˆì´ì…˜ ìƒì„± ì¤‘...")
    narr_dir = os.path.join(output_dir, "narrations")
    os.makedirs(narr_dir, exist_ok=True)
    narration_paths = generate_narrations(slides, narr_dir, voice)

    # Phase 2: ì”¬ í´ë¦½ ìƒì„± (GIF ìƒë‹¨ 55% + ë¸”ë£¨ë°” í•˜ë‹¨ 45% + ì˜¤ë²„ë ˆì´)
    if progress_callback:
        progress_callback(0.25, "ì”¬ í´ë¦½ ìƒì„± ì¤‘...")
    scene_clips = []
    media_stats = {"gif": 0, "video": 0, "image": 0, "none": 0}
    for i, (m_bytes, m_info) in enumerate(media_data):
        narr_path = narration_paths[i] if i < len(narration_paths) else ""
        narr_dur = get_audio_duration(narr_path)
        slide_dur = narr_dur + SLIDE_PADDING
        overlay = overlay_images[i] if i < len(overlay_images) else None

        # ë¯¸ë””ì–´ ì§„ë‹¨ ë¡œê·¸
        if m_bytes and m_info:
            src = f"{m_info['type']}/{m_info.get('source', '?')}"
            media_stats[m_info.get("type", "none")] = media_stats.get(m_info.get("type", "none"), 0) + 1
            logger.info(f"ì”¬ {i}: ë¯¸ë””ì–´ {src} ({len(m_bytes)} bytes)")
        else:
            media_stats["none"] += 1
            logger.info(f"ì”¬ {i}: ë¯¸ë””ì–´ ì—†ìŒ â†’ ë¸Œëœë“œ ë°°ê²½")

        try:
            scene = _media_to_scene_clip(m_bytes, m_info, overlay, slide_dur)
            scene_clips.append(scene)
        except Exception as e:
            logger.warning(f"ì”¬ í´ë¦½ ì‹¤íŒ¨ slide_{i}: {e}")
            scene = _media_to_scene_clip(None, None, overlay, slide_dur)
            scene_clips.append(scene)

        if progress_callback:
            progress_callback(0.25 + (i / len(media_data)) * 0.15,
                              f"ì”¬ {i + 1}/{len(media_data)} ìƒì„± ì™„ë£Œ")

    logger.info(f"ë¯¸ë””ì–´ í†µê³„: {media_stats}")

    # Phase 3: ì˜ìƒ í•©ì„±
    if progress_callback:
        progress_callback(0.40, "ì˜ìƒ í•©ì„± ì¤‘...")
    video_path = os.path.join(output_dir, "reel.mp4")
    compose_reel(
        scene_clips=scene_clips,
        narration_paths=narration_paths,
        output_path=video_path,
        include_intro=include_intro,
        include_bumper=include_bumper,
        progress_callback=lambda s, t, m: (
            progress_callback(0.40 + (s / t) * 0.50, m) if progress_callback else None
        ),
    )

    # Phase 4: ê²°ê³¼
    if progress_callback:
        progress_callback(0.95, "ë§ˆë¬´ë¦¬ ì¤‘...")

    video_bytes = Path(video_path).read_bytes()
    try:
        vc = VideoFileClip(video_path)
        duration = vc.duration
        vc.close()
    except Exception:
        duration = 0.0

    if progress_callback:
        progress_callback(1.0, "ì™„ë£Œ!")

    return {
        "video_path": video_path,
        "video_bytes": video_bytes,
        "narration_paths": narration_paths,
        "duration": duration,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë ˆê±°ì‹œ í˜¸í™˜ (ì •ì  í”„ë ˆì„ ì´ë¯¸ì§€)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_reel_legacy(
    slides: list[dict],
    frame_images: list[bytes],
    output_dir: str | None = None,
    voice: str = DEFAULT_VOICE,
    include_intro: bool = False,
    include_bumper: bool = True,
    progress_callback=None,
) -> dict:
    """ë ˆê±°ì‹œ: ì •ì  í”„ë ˆì„ ì´ë¯¸ì§€ ê¸°ë°˜ ë¦´ìŠ¤ ìƒì„±."""
    if output_dir is None:
        output_dir = tempfile.mkdtemp(prefix="reel_")
    os.makedirs(output_dir, exist_ok=True)

    if progress_callback:
        progress_callback(0.0, "ë‚˜ë ˆì´ì…˜ ìƒì„± ì¤‘...")
    narr_dir = os.path.join(output_dir, "narrations")
    os.makedirs(narr_dir, exist_ok=True)
    narration_paths = generate_narrations(slides, narr_dir, voice)

    if progress_callback:
        progress_callback(0.33, "ì˜ìƒ í•©ì„± ì¤‘...")

    scene_clips = []
    for i, img_bytes in enumerate(frame_images):
        narr_path = narration_paths[i] if i < len(narration_paths) else ""
        narr_dur = get_audio_duration(narr_path)
        slide_dur = narr_dur + SLIDE_PADDING
        scene_clips.append(_image_bytes_to_clip(img_bytes, slide_dur))

    video_path = os.path.join(output_dir, "reel.mp4")
    compose_reel(
        scene_clips=scene_clips,
        narration_paths=narration_paths,
        output_path=video_path,
        include_intro=include_intro,
        include_bumper=include_bumper,
        progress_callback=lambda s, t, m: (
            progress_callback(0.33 + (s / t) * 0.62, m) if progress_callback else None
        ),
    )

    if progress_callback:
        progress_callback(0.95, "ë§ˆë¬´ë¦¬ ì¤‘...")

    video_bytes = Path(video_path).read_bytes()
    try:
        vc = VideoFileClip(video_path)
        duration = vc.duration
        vc.close()
    except Exception:
        duration = 0.0

    if progress_callback:
        progress_callback(1.0, "ì™„ë£Œ!")

    return {
        "video_path": video_path,
        "video_bytes": video_bytes,
        "narration_paths": narration_paths,
        "duration": duration,
    }
