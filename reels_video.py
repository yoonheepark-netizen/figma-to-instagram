"""ë¦´ìŠ¤ ì˜ìƒ í•©ì„± ëª¨ë“ˆ â€” 1ë¶„ê±´ê°•í†¡.

ì°¸ê³  ì˜ìƒ ìŠ¤íƒ€ì¼: ìƒë‹¨ 55% GIF/ì´ë¯¸ì§€ + í•˜ë‹¨ 45% ë¸Œëœë“œ ë¸”ë£¨ í…ìŠ¤íŠ¸.
GIFëŠ” ê½‰ ì±„ìš°ì§€ ì•Šê³  ìƒë‹¨ ì˜ì—­ì— ë°°ì¹˜. ì˜¤í”„ë‹ ì—†ì´ ë³¸ë¡ ë¶€í„° ì‹œì‘.
ë‚˜ë ˆì´ì…˜(edge-tts) ê¸°ë°˜ ë™ì  ì”¬ êµ¬ì„± â†’ BUMPER.mov ì—°ê²°.
"""
from __future__ import annotations

import asyncio
import io
import logging
import os
import tempfile
from pathlib import Path

import edge_tts
import numpy as np
from moviepy import (
    AudioFileClip,
    CompositeVideoClip,
    ImageClip,
    VideoFileClip,
    concatenate_videoclips,
    CompositeAudioClip,
)
from PIL import Image

from sfx import generate_sfx

logger = logging.getLogger(__name__)

# â”€â”€ ê²½ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_ASSETS_DIR = os.path.join(os.path.dirname(__file__), "assets", "1min_health")
INTRO_PATH = os.path.join(_ASSETS_DIR, "INTRO.mp4")
BUMPER_PATH = os.path.join(_ASSETS_DIR, "BUMPER.mov")

# â”€â”€ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
W, H = 1080, 1920  # 9:16
FPS = 30
TRANSITION_DUR = 0.4
SLIDE_PADDING = 0.5
MEDIA_RATIO = 0.55  # GIF/ì´ë¯¸ì§€ê°€ ì°¨ì§€í•˜ëŠ” ìƒë‹¨ ë¹„ìœ¨ (ì°¸ê³  ì˜ìƒ ê¸°ì¤€)
MEDIA_H = int(H * MEDIA_RATIO)  # ~1056px
BRAND_BLUE = (43, 91, 224)

# â”€â”€ temp íŒŒì¼ ì¶”ì  (MoviePy ë Œë”ë§ ì™„ë£Œ í›„ ì •ë¦¬) â”€â”€â”€â”€â”€â”€â”€â”€
_temp_files: list[str] = []


def _track_temp(path: str) -> str:
    """temp íŒŒì¼ ê²½ë¡œë¥¼ ì¶”ì  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€."""
    _temp_files.append(path)
    return path


def _cleanup_temp_files():
    """ì¶”ì ëœ temp íŒŒì¼ ì¼ê´„ ì •ë¦¬."""
    for path in _temp_files:
        try:
            os.unlink(path)
        except Exception:
            pass
    _temp_files.clear()

# â”€â”€ ìŒì„± í”„ë¦¬ì…‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
VOICES = {
    "ì—¬ì„± (ì„ íˆ)": "ko-KR-SunHiNeural",
    "ë‚¨ì„± (í˜„ìˆ˜)": "ko-KR-HyunsuMultilingualNeural",
    "ë‚¨ì„± (ì¸ì¤€)": "ko-KR-InJoonNeural",
}
DEFAULT_VOICE = "ko-KR-HyunsuMultilingualNeural"

# ìŒì„±ë³„ ìµœì  rate/pitch ì„¤ì • (ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬)
_VOICE_PRESETS = {
    "ko-KR-SunHiNeural": {"rate": "-8%", "pitch": "+5Hz"},
    "ko-KR-HyunsuMultilingualNeural": {"rate": "-5%", "pitch": "+0Hz"},
    "ko-KR-InJoonNeural": {"rate": "-5%", "pitch": "+0Hz"},
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë‚˜ë ˆì´ì…˜ ìƒì„± (edge-tts)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _preprocess_narration(text: str) -> str:
    """ë‚˜ë ˆì´ì…˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ â€” ìì—°ìŠ¤ëŸ¬ìš´ TTSë¥¼ ìœ„í•œ ë³´ì •.

    - ì´ëª¨ì§€ ì œê±° (TTSê°€ ì½ìœ¼ë©´ ì–´ìƒ‰)
    - ã…‹ã…‹ ë“± ì›ƒìŒ í‘œí˜„ ì œê±°
    - ë§ˆì¹¨í‘œ ë’¤ ì‰¼í‘œ ì¶”ê°€ (ìì—°ìŠ¤ëŸ¬ìš´ í˜¸í¡)
    """
    import re
    # ì´ëª¨ì§€ ì œê±°
    text = re.sub(r'[\U0001F600-\U0001F9FF\U00002702-\U000027B0'
                  r'\U0001F1E0-\U0001F1FF\U00002600-\U000026FF'
                  r'\U0000FE00-\U0000FE0F\U0001FA00-\U0001FAFF]+', '', text)
    # ã…‹ã…‹, ã„·ã„· ë“± ì œê±°
    text = re.sub(r'[ã…‹ã…ã„·ã… ã…œ]{2,}', '', text)
    # ì—°ì† ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    return text


async def _generate_narration_async(text: str, output_path: str,
                                     voice: str = DEFAULT_VOICE) -> str:
    text = _preprocess_narration(text)
    preset = _VOICE_PRESETS.get(voice, {})
    communicate = edge_tts.Communicate(
        text, voice,
        rate=preset.get("rate", "-5%"),
        pitch=preset.get("pitch", "+0Hz"),
    )
    await communicate.save(output_path)
    return output_path


def generate_narration(text: str, output_path: str,
                       voice: str = DEFAULT_VOICE) -> str:
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            import nest_asyncio
            nest_asyncio.apply()
            loop.run_until_complete(
                _generate_narration_async(text, output_path, voice))
        else:
            loop.run_until_complete(
                _generate_narration_async(text, output_path, voice))
    except RuntimeError:
        asyncio.run(_generate_narration_async(text, output_path, voice))
    return output_path


def generate_narrations(slides: list[dict], tmp_dir: str,
                        voice: str = DEFAULT_VOICE) -> list[str]:
    paths = []
    for i, slide in enumerate(slides):
        narration = slide.get("narration", "")
        if not narration.strip():
            paths.append("")
            continue
        out = os.path.join(tmp_dir, f"narration_{i}.mp3")
        try:
            generate_narration(narration, out, voice)
            paths.append(out)
            logger.info(f"ë‚˜ë ˆì´ì…˜ ìƒì„± ì™„ë£Œ: slide_{i} â†’ {out}")
        except Exception as e:
            logger.error(f"ë‚˜ë ˆì´ì…˜ ìƒì„± ì‹¤íŒ¨ (slide_{i}): {e}")
            paths.append("")
    return paths


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì˜¤ë””ì˜¤ ìœ í‹¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_audio_duration(audio_path: str) -> float:
    if not audio_path or not os.path.exists(audio_path):
        return 3.0
    try:
        clip = AudioFileClip(audio_path)
        dur = clip.duration
        clip.close()
        return dur
    except Exception:
        return 3.0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í´ë¦½ ìœ í‹¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fit_clip_to_reel(clip):
    """ì˜ìƒ/ì´ë¯¸ì§€ í´ë¦½ì„ 1080Ã—1920ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ+í¬ë¡­."""
    cw, ch = clip.size
    target_ratio = W / H

    if cw / ch > target_ratio:
        new_h = H
        new_w = int(cw * (H / ch))
    else:
        new_w = W
        new_h = int(ch * (W / cw))

    resized = clip.resized((new_w, new_h))
    x_center = new_w / 2
    y_center = new_h / 2
    cropped = resized.cropped(
        x1=x_center - W / 2, y1=y_center - H / 2,
        x2=x_center + W / 2, y2=y_center + H / 2,
    )
    return cropped


def _letterbox_landscape(clip, bg_color=(43, 91, 224)):
    """ê°€ë¡œ ì˜ìƒì„ ì„¸ë¡œ í”„ë ˆì„ ì•ˆì— ë ˆí„°ë°•ìŠ¤ë¡œ ë°°ì¹˜ (ì›ë³¸ ë¹„ìœ¨ ìœ ì§€).

    ê°€ë¡œ 1920Ã—1080 â†’ ì„¸ë¡œ 1080Ã—1920 ì•ˆì—ì„œ:
      - ì˜ìƒì„ ê°€ë¡œí­ 1080ì— ë§ê²Œ ì¶•ì†Œ (1080Ã—607)
      - ìƒí•˜ ë¸Œëœë“œ ë¸”ë£¨ ë°°ê²½ìœ¼ë¡œ íŒ¨ë”©
    """
    cw, ch = clip.size

    # ê°€ë¡œí­ = Wì— ë§ì¶”ê³ , ì„¸ë¡œëŠ” ë¹„ìœ¨ ìœ ì§€
    scale = W / cw
    new_w = W
    new_h = int(ch * scale)
    resized = clip.resized((new_w, new_h))

    # ë¸Œëœë“œ ë¸”ë£¨ ë°°ê²½
    bg_arr = np.full((H, W, 3), bg_color, dtype=np.uint8)
    bg_clip = ImageClip(bg_arr).with_duration(clip.duration)

    # ì„¸ë¡œ ì¤‘ì•™ ë°°ì¹˜
    y_offset = (H - new_h) // 2
    final = CompositeVideoClip(
        [bg_clip, resized.with_position(("center", y_offset))],
        size=(W, H),
    ).with_duration(clip.duration)

    # ì›ë³¸ ì˜¤ë””ì˜¤ ìœ ì§€
    if clip.audio is not None:
        final = final.with_audio(clip.audio)

    return final


def _image_bytes_to_clip(img_bytes: bytes, duration: float) -> ImageClip:
    """PNG bytes â†’ MoviePy ImageClip (RGB)."""
    img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
    arr = np.array(img)
    return ImageClip(arr).with_duration(duration)


def _overlay_png_to_clip(png_bytes: bytes, duration: float) -> ImageClip:
    """íˆ¬ëª… PNG bytes â†’ MoviePy ImageClip (RGBA ë§ˆìŠ¤í¬ ì§€ì›)."""
    img = Image.open(io.BytesIO(png_bytes)).convert("RGBA")
    arr = np.array(img)
    # RGBAì—ì„œ RGB + mask ë¶„ë¦¬
    rgb = arr[:, :, :3]
    alpha = arr[:, :, 3] / 255.0  # 0~1 ë²”ìœ„
    clip = ImageClip(rgb).with_duration(duration)
    mask = ImageClip(alpha, is_mask=True).with_duration(duration)
    clip = clip.with_mask(mask)
    return clip


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GIF/ì˜ìƒ â†’ ìƒë‹¨ 55% ì”¬ í´ë¦½ (ì°¸ê³  ì˜ìƒ ìŠ¤íƒ€ì¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fit_to_area(clip, target_w: int, target_h: int):
    """í´ë¦½ì„ target_w Ã— target_h ì˜ì—­ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ+í¬ë¡­."""
    cw, ch = clip.size
    target_ratio = target_w / target_h

    if cw / ch > target_ratio:
        new_h = target_h
        new_w = int(cw * (target_h / ch))
    else:
        new_w = target_w
        new_h = int(ch * (target_w / cw))

    resized = clip.resized((new_w, new_h))
    x_center = new_w / 2
    y_center = new_h / 2
    return resized.cropped(
        x1=x_center - target_w / 2, y1=y_center - target_h / 2,
        x2=x_center + target_w / 2, y2=y_center + target_h / 2,
    )


def _load_video_clip(media_bytes: bytes, media_info: dict, duration: float):
    """ë¯¸ë””ì–´ bytes â†’ ë£¨í•‘ VideoFileClip (temp íŒŒì¼ ì¶”ì ).

    ì£¼ì˜: temp íŒŒì¼ì€ MoviePy ë Œë”ë§ì´ ëë‚  ë•Œê¹Œì§€ ìœ ì§€!
    """
    is_mp4 = media_info.get("mp4_url", "").endswith(".mp4") or b"ftyp" in media_bytes[:20]
    suffix = ".mp4" if is_mp4 else ".gif"

    tmp = tempfile.NamedTemporaryFile(suffix=suffix, delete=False)
    tmp.write(media_bytes)
    tmp.close()
    _track_temp(tmp.name)

    clip = VideoFileClip(tmp.name)
    # ë£¨í•‘ (ìµœëŒ€ 3íšŒ â€” ë©”ëª¨ë¦¬ ì ˆì•½)
    if clip.duration < duration:
        n_loops = min(int(duration / clip.duration) + 1, 3)
        clip = concatenate_videoclips([clip] * n_loops).subclipped(0, duration)
    else:
        clip = clip.subclipped(0, min(clip.duration, duration))
    return clip


def _media_to_scene_clip(media_bytes: bytes, media_info: dict,
                          overlay_png: bytes | None, duration: float):
    """ë¯¸ë””ì–´ + ì˜¤ë²„ë ˆì´ â†’ 1ë¶„ê±´ê°•í†¡ ìŠ¤íƒ€ì¼ ì”¬ í´ë¦½.

    ë ˆì´ì•„ì›ƒ (ì°¸ê³  ì˜ìƒ ê¸°ì¤€):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ [ğŸ”´í†¡]   n/N    â”‚  â† ì˜¤ë²„ë ˆì´ (ë¡œê³ , ë²ˆí˜¸)
    â”‚                 â”‚
    â”‚  GIF/ì´ë¯¸ì§€     â”‚  â† ìƒë‹¨ 55% (MEDIA_H px)
    â”‚  (ì›ë³¸ë¹„ìœ¨í¬ë¡­)  â”‚
    â”‚                 â”‚
    â”œâ”€ ê·¸ë¼ë°ì´ì…˜ â”€â”€â”€â”€â”¤
    â”‚  â–  ë¸”ë£¨ â– â– â– â– â–   â”‚  â† í•˜ë‹¨ 45% (ë¸Œëœë“œ ë¸”ë£¨)
    â”‚  display_text   â”‚  â† ì˜¤ë²„ë ˆì´ í…ìŠ¤íŠ¸
    â”‚  @1ë¶„ê±´ê°•í†¡     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    media_type = media_info.get("type", "image") if media_info else "none"

    # 1. ë¸Œëœë“œ ë¸”ë£¨ ë°°ê²½ (ì „ì²´)
    bg_arr = np.full((H, W, 3), BRAND_BLUE, dtype=np.uint8)
    bg_clip = ImageClip(bg_arr).with_duration(duration)

    layers = [bg_clip]

    # 2. ë¯¸ë””ì–´ í´ë¦½ â†’ ìƒë‹¨ MEDIA_H ì˜ì—­ì— ë°°ì¹˜
    media_clip = None
    try:
        if media_type in ("gif", "video") and media_bytes:
            raw_clip = _load_video_clip(media_bytes, media_info, duration)
            media_clip = _fit_to_area(raw_clip, W, MEDIA_H)
        elif media_type == "image" and media_bytes:
            img = Image.open(io.BytesIO(media_bytes)).convert("RGB")
            img = _fit_cover_pil(img, W, MEDIA_H)
            media_clip = ImageClip(np.array(img)).with_duration(duration)
    except Exception as e:
        logger.warning(f"ë¯¸ë””ì–´ í´ë¦½ ìƒì„± ì‹¤íŒ¨: {e}")

    if media_clip is not None:
        layers.append(media_clip.with_position((0, 0)))

    # 3. í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´
    if overlay_png:
        overlay_clip = _overlay_png_to_clip(overlay_png, duration)
        layers.append(overlay_clip)

    scene = CompositeVideoClip(layers, size=(W, H)).with_duration(duration)
    return scene


def _fit_cover_pil(img: Image.Image, w: int, h: int) -> Image.Image:
    """PIL ì´ë¯¸ì§€ë¥¼ wÃ—hì— ë§ê²Œ ì»¤ë²„ í¬ë¡­."""
    pw, ph = img.size
    target_ratio = w / h
    if pw / ph > target_ratio:
        new_w = int(ph * target_ratio)
        left = (pw - new_w) // 2
        img = img.crop((left, 0, left + new_w, ph))
    else:
        new_h = int(pw / target_ratio)
        top = (ph - new_h) // 2
        img = img.crop((0, top, pw, top + new_h))
    return img.resize((w, h), Image.LANCZOS)


def _solid_color_clip(duration: float, color=None):
    """ë‹¨ìƒ‰ ë°°ê²½ í´ë¦½ (í´ë°±ìš©)."""
    c = color or BRAND_BLUE
    arr = np.full((H, W, 3), c, dtype=np.uint8)
    return ImageClip(arr).with_duration(duration)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìŠ¤ì™€ì´í”„ ì „í™˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _swipe_transition(clip1, clip2, trans_dur: float = TRANSITION_DUR):
    w = W

    def pos1(t):
        progress = t / trans_dur
        return (-w * progress, 0)

    def pos2(t):
        progress = t / trans_dur
        return (w - w * progress, 0)

    c1 = clip1.with_duration(trans_dur).with_position(pos1)
    c2 = clip2.with_duration(trans_dur).with_position(pos2)
    return CompositeVideoClip([c1, c2], size=(W, H)).with_duration(trans_dur)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ í•©ì„± (GIF/ì˜ìƒ ë°°ê²½ + í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def compose_reel(
    scene_clips: list,
    narration_paths: list[str],
    output_path: str,
    include_intro: bool = False,
    include_bumper: bool = True,
    progress_callback=None,
) -> str:
    """ë¦´ìŠ¤ ì˜ìƒ í•©ì„±.

    Args:
        scene_clips: ì™„ì„±ëœ ì”¬ í´ë¦½ ë¦¬ìŠ¤íŠ¸ (GIF+ë¸”ë£¨ë°”+í…ìŠ¤íŠ¸ í•©ì„± ì™„ë£Œ)
        narration_paths: ë‚˜ë ˆì´ì…˜ MP3 ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        output_path: ì¶œë ¥ MP4 ê²½ë¡œ
        include_intro: ì¸íŠ¸ë¡œ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸: False â€” ë³¸ë¡ ë¶€í„° ì‹œì‘)
    """

    def _progress(step, total, msg):
        if progress_callback:
            progress_callback(step, total, msg)
        logger.info(f"[{step}/{total}] {msg}")

    total_steps = len(scene_clips) + 3
    current_step = 0

    composed_slides = []
    slide_audios = []
    cumulative_time = 0.0

    # INTRO (ê¸°ë³¸ ë¹„í™œì„± â€” ë³¸ë¡ ë¶€í„° ì‹œì‘)
    if include_intro and os.path.exists(INTRO_PATH):
        current_step += 1
        _progress(current_step, total_steps, "ì¸íŠ¸ë¡œ ì˜ìƒ ë¡œë“œ ì¤‘...")
        intro_clip = VideoFileClip(INTRO_PATH)
        iw, ih = intro_clip.size
        if iw > ih:
            intro_clip = _letterbox_landscape(intro_clip)
        elif (iw, ih) != (W, H):
            intro_clip = _fit_clip_to_reel(intro_clip)
        composed_slides.append(intro_clip)
        cumulative_time += intro_clip.duration
    else:
        current_step += 1

    # ì”¬ í´ë¦½ ë°°ì¹˜ + ë‚˜ë ˆì´ì…˜ ë™ê¸°í™”
    for i, scene_clip in enumerate(scene_clips):
        current_step += 1
        _progress(current_step, total_steps, f"ì”¬ {i + 1}/{len(scene_clips)} ë°°ì¹˜ ì¤‘...")

        narr_path = narration_paths[i] if i < len(narration_paths) else ""
        narr_dur = get_audio_duration(narr_path)
        slide_dur = narr_dur + SLIDE_PADDING

        # ì”¬ duration ì¡°ì •
        if hasattr(scene_clip, 'duration') and scene_clip.duration and scene_clip.duration >= slide_dur:
            final_scene = scene_clip.subclipped(0, slide_dur)
        else:
            final_scene = scene_clip.with_duration(slide_dur)

        composed_slides.append(final_scene)

        if narr_path and os.path.exists(narr_path):
            audio = AudioFileClip(narr_path)
            slide_audios.append((audio, cumulative_time))

        cumulative_time += slide_dur

    # BUMPER
    if include_bumper and os.path.exists(BUMPER_PATH):
        current_step += 1
        _progress(current_step, total_steps, "ë²”í¼ ì˜ìƒ ë¡œë“œ ì¤‘...")
        bumper_clip = VideoFileClip(BUMPER_PATH)
        bw, bh = bumper_clip.size
        if (bw, bh) != (W, H):
            if bw > bh:
                bumper_clip = _letterbox_landscape(bumper_clip)
            else:
                bumper_clip = _fit_clip_to_reel(bumper_clip)
        composed_slides.append(bumper_clip)
    else:
        current_step += 1

    # í´ë¦½ ì—°ê²° (ë‹¨ìˆœ ì—°ê²° â€” ë©”ëª¨ë¦¬ ì ˆì•½)
    if len(composed_slides) == 0:
        raise ValueError("í•©ì„±í•  ìŠ¬ë¼ì´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

    if len(composed_slides) == 1:
        final_video = composed_slides[0]
    else:
        final_video = concatenate_videoclips(composed_slides, method="chain")

    # ì˜¤ë””ì˜¤ í•©ì„± (ë‚˜ë ˆì´ì…˜ë§Œ â€” SFX ìƒëµìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
    if slide_audios:
        all_audio_parts = [audio.with_start(start) for audio, start in slide_audios]
        combined_audio = CompositeAudioClip(all_audio_parts)
        if final_video.audio is not None:
            combined_audio = CompositeAudioClip([final_video.audio, combined_audio])
        final_video = final_video.with_audio(combined_audio)

    # ë‚´ë³´ë‚´ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½: threads=1, ultrafast)
    current_step = total_steps
    _progress(current_step, total_steps, "MP4 ë‚´ë³´ë‚´ê¸° ì¤‘...")

    final_video.write_videofile(
        output_path, fps=FPS, codec="libx264", audio_codec="aac",
        threads=1, preset="ultrafast", logger=None,
    )

    # ì •ë¦¬
    final_video.close()
    for clip in composed_slides:
        try:
            clip.close()
        except Exception:
            pass
    for audio, _ in slide_audios:
        try:
            audio.close()
        except Exception:
            pass

    # ë©”ëª¨ë¦¬ í•´ì œ
    import gc
    gc.collect()

    # GIF/ì˜ìƒ temp íŒŒì¼ ì •ë¦¬ (ë Œë”ë§ ì™„ë£Œ í›„ì—ë§Œ!)
    _cleanup_temp_files()

    logger.info(f"ë¦´ìŠ¤ ì˜ìƒ ìƒì„± ì™„ë£Œ: {output_path}")
    return output_path


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í†µí•© íŒŒì´í”„ë¼ì¸ (GIF/ì˜ìƒ ë°°ê²½)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_reel(
    slides: list[dict],
    media_data: list[tuple[bytes | None, dict | None]],
    overlay_images: list[bytes],
    output_dir: str | None = None,
    voice: str = DEFAULT_VOICE,
    include_intro: bool = False,
    include_bumper: bool = True,
    progress_callback=None,
) -> dict:
    """ë¦´ìŠ¤ ìƒì„± í†µí•© íŒŒì´í”„ë¼ì¸ (GIF ìƒë‹¨ 55% + ë¸”ë£¨ë°” í•˜ë‹¨ 45%).

    Args:
        slides: ìŠ¤í¬ë¦½íŠ¸ ìŠ¬ë¼ì´ë“œ ë¦¬ìŠ¤íŠ¸
        media_data: [(bytes, metadata), ...] ìŠ¬ë¼ì´ë“œë³„ ë¯¸ë””ì–´ ë°ì´í„°
        overlay_images: í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´ PNG bytes
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        voice: TTS ìŒì„± ID
        include_intro: ì¸íŠ¸ë¡œ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ False â€” ë³¸ë¡ ë¶€í„° ì‹œì‘)
    """
    if output_dir is None:
        output_dir = tempfile.mkdtemp(prefix="reel_")
    os.makedirs(output_dir, exist_ok=True)

    # Phase 1: ë‚˜ë ˆì´ì…˜
    if progress_callback:
        progress_callback(0.0, "ë‚˜ë ˆì´ì…˜ ìƒì„± ì¤‘...")
    narr_dir = os.path.join(output_dir, "narrations")
    os.makedirs(narr_dir, exist_ok=True)
    narration_paths = generate_narrations(slides, narr_dir, voice)

    # Phase 2: ì”¬ í´ë¦½ ìƒì„± (GIF ìƒë‹¨ 55% + ë¸”ë£¨ë°” í•˜ë‹¨ 45% + ì˜¤ë²„ë ˆì´)
    if progress_callback:
        progress_callback(0.25, "ì”¬ í´ë¦½ ìƒì„± ì¤‘...")
    scene_clips = []
    for i, (m_bytes, m_info) in enumerate(media_data):
        narr_path = narration_paths[i] if i < len(narration_paths) else ""
        narr_dur = get_audio_duration(narr_path)
        slide_dur = narr_dur + SLIDE_PADDING
        overlay = overlay_images[i] if i < len(overlay_images) else None

        try:
            scene = _media_to_scene_clip(m_bytes, m_info, overlay, slide_dur)
            scene_clips.append(scene)
            src = f"{m_info['type']}/{m_info.get('source', '?')}" if m_info else "ë¸Œëœë“œ ë°°ê²½"
            logger.info(f"ì”¬ í´ë¦½: slide_{i} ({src})")
        except Exception as e:
            logger.warning(f"ì”¬ í´ë¦½ ì‹¤íŒ¨ slide_{i}: {e}")
            # í´ë°±: ì˜¤ë²„ë ˆì´ë§Œ ìˆìœ¼ë©´ ë¸”ë£¨ ë°°ê²½ + ì˜¤ë²„ë ˆì´
            scene = _media_to_scene_clip(None, None, overlay, slide_dur)
            scene_clips.append(scene)

        if progress_callback:
            progress_callback(0.25 + (i / len(media_data)) * 0.15,
                              f"ì”¬ {i + 1}/{len(media_data)} ìƒì„± ì™„ë£Œ")

    # Phase 3: ì˜ìƒ í•©ì„±
    if progress_callback:
        progress_callback(0.40, "ì˜ìƒ í•©ì„± ì¤‘...")
    video_path = os.path.join(output_dir, "reel.mp4")
    compose_reel(
        scene_clips=scene_clips,
        narration_paths=narration_paths,
        output_path=video_path,
        include_intro=include_intro,
        include_bumper=include_bumper,
        progress_callback=lambda s, t, m: (
            progress_callback(0.40 + (s / t) * 0.50, m) if progress_callback else None
        ),
    )

    # Phase 4: ê²°ê³¼
    if progress_callback:
        progress_callback(0.95, "ë§ˆë¬´ë¦¬ ì¤‘...")

    video_bytes = Path(video_path).read_bytes()
    try:
        vc = VideoFileClip(video_path)
        duration = vc.duration
        vc.close()
    except Exception:
        duration = 0.0

    if progress_callback:
        progress_callback(1.0, "ì™„ë£Œ!")

    return {
        "video_path": video_path,
        "video_bytes": video_bytes,
        "narration_paths": narration_paths,
        "duration": duration,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë ˆê±°ì‹œ í˜¸í™˜ (ì •ì  í”„ë ˆì„ ì´ë¯¸ì§€)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_reel_legacy(
    slides: list[dict],
    frame_images: list[bytes],
    output_dir: str | None = None,
    voice: str = DEFAULT_VOICE,
    include_intro: bool = False,
    include_bumper: bool = True,
    progress_callback=None,
) -> dict:
    """ë ˆê±°ì‹œ: ì •ì  í”„ë ˆì„ ì´ë¯¸ì§€ ê¸°ë°˜ ë¦´ìŠ¤ ìƒì„±."""
    if output_dir is None:
        output_dir = tempfile.mkdtemp(prefix="reel_")
    os.makedirs(output_dir, exist_ok=True)

    if progress_callback:
        progress_callback(0.0, "ë‚˜ë ˆì´ì…˜ ìƒì„± ì¤‘...")
    narr_dir = os.path.join(output_dir, "narrations")
    os.makedirs(narr_dir, exist_ok=True)
    narration_paths = generate_narrations(slides, narr_dir, voice)

    if progress_callback:
        progress_callback(0.33, "ì˜ìƒ í•©ì„± ì¤‘...")

    scene_clips = []
    for i, img_bytes in enumerate(frame_images):
        narr_path = narration_paths[i] if i < len(narration_paths) else ""
        narr_dur = get_audio_duration(narr_path)
        slide_dur = narr_dur + SLIDE_PADDING
        scene_clips.append(_image_bytes_to_clip(img_bytes, slide_dur))

    video_path = os.path.join(output_dir, "reel.mp4")
    compose_reel(
        scene_clips=scene_clips,
        narration_paths=narration_paths,
        output_path=video_path,
        include_intro=include_intro,
        include_bumper=include_bumper,
        progress_callback=lambda s, t, m: (
            progress_callback(0.33 + (s / t) * 0.62, m) if progress_callback else None
        ),
    )

    if progress_callback:
        progress_callback(0.95, "ë§ˆë¬´ë¦¬ ì¤‘...")

    video_bytes = Path(video_path).read_bytes()
    try:
        vc = VideoFileClip(video_path)
        duration = vc.duration
        vc.close()
    except Exception:
        duration = 0.0

    if progress_callback:
        progress_callback(1.0, "ì™„ë£Œ!")

    return {
        "video_path": video_path,
        "video_bytes": video_bytes,
        "narration_paths": narration_paths,
        "duration": duration,
    }
